<!-- ========================= lecture_27.html ========================= -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Lecture 26 - Logistic Regression</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!-- MathJax for rendering formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root{--bg:#fafafa;--ink:#222;--brand:#004488;--accent:#0aa2ff;--soft:#ffffff}
    body{font-family:Arial,Helvetica,sans-serif;margin:24px 5vw;background:var(--bg);color:var(--ink);line-height:1.6}
    .nav{display:flex;flex-wrap:wrap;gap:10px;margin:8px 0 24px}
    .nav a{padding:8px 12px;border-radius:8px;background:#fff;border:1px solid #e5e7eb;text-decoration:none;color:#0d6efd}
    .nav a[aria-current="page"]{background:#e7f1ff;border-color:#b6d4fe;color:#084298;font-weight:700}
    h1,h2,h3{color:var(--brand)}
    .card{background:#fff;border-radius:12px;padding:16px;margin:14px 0;box-shadow:0 2px 8px rgba(0,0,0,.06);border-left:5px solid var(--accent)}
    .grid{display:grid;gap:14px}
    @media(min-width:1000px){.grid.cols-2{grid-template-columns:1fr 1fr}.grid.cols-3{grid-template-columns:1fr 1fr 1fr}}
    code{background:#eef2f7;border-radius:4px;padding:2px 6px;display:block;white-space:pre-wrap}
    pre{background:#0b1220;color:#e6eef8;padding:12px;border-radius:8px;overflow:auto}
    .note{background:#fff8e6;border-left:5px solid #ffb84d;padding:10px;border-radius:6px}
    .try{border-left-color:#20c997}
    table{width:100%;border-collapse:collapse;background:#fff;border-radius:10px;overflow:hidden}
    th,td{padding:10px;border-bottom:1px solid #e9ecef;text-align:left}
    th{background:#f1f5f9}
    .math{display:block;padding:8px 10px;background:#f4fbff;border-radius:6px;border:1px solid #e3f0ff}
  </style>
</head>
<body>
  <nav class="nav">
    <a href="https://surnamnarendra.github.io/Fundamentals-of-Machine-Learning/" aria-current="page">Home</a>
    <a href="lecture_17.html">L21</a>
    <a href="lecture_18.html">L22</a>
    <a href="lecture_19.html">L23</a>
    <a href="lecture_20.html">L24</a>
    <a href="lecture_21.html">L25</a>
    <a href="lecture_22.html">L26</a>
    <a href="lecture_23.html">L27</a>
    <a href="lecture_24.html">L28</a>
    <a href="lecture_25.html">L29</a>
    <a href="lecture_26.html">L30</a>
    <a href="lecture_27.html" aria-current="page">L31</a>
  </nav>

  <h1>Lecture 26: Logistic Regression</h1>

  <div class="card">
    <h2>1. What &amp; Why</h2>
    <p>
      Logistic Regression is a parametric model for <b>binary classification</b> (can be extended to multinomial).
      It models the probability that the target belongs to class 1 given input features.
      It's simple, fast, and interpretable — a frequent first-choice baseline for classification tasks.
    </p>
  </div>

  <div class="card">
    <h2>2. Mathematical formulation</h2>

    <p>Start with a linear combination (score):</p>
    <div class="math">
      \( z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n \)
    </div>

    <p>Pass the score through the sigmoid (logistic) function to obtain a probability:</p>
    <div class="math">
      \( \sigma(z) = \dfrac{1}{1 + e^{-z}} \),\quad so \quad
      \( P(Y=1 \mid X) = \sigma(z) \).
    </div>

    <p>Decision rule (default threshold 0.5):</p>
    <div class="math">
      \( \hat{y} = \begin{cases} 1 & \text{if } \sigma(z) \ge 0.5 \\ 0 & \text{otherwise} \end{cases} \)
    </div>

    <h3>Log-loss (cross-entropy) objective</h3>
    <div class="math">
      \( J(\beta) = -\dfrac{1}{m}\sum_{i=1}^m \big[ y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)}) \big] \)
    </div>

    <p>
      We minimize \(J(\beta)\) — usually via numerical optimization (Gradient Descent, L-BFGS, or other solvers).
      Regularization (L1 / L2) is commonly added: \( J_{reg} = J + \lambda \| \beta \| \).
    </p>
  </div>

  <div class="card">
    <h2>3. Interpretation</h2>
    <ul>
      <li>Each coefficient \(\beta_j\) represents the log-odds change per unit increase in \(x_j\):</li>
      <div class="math">\( \log\frac{P(Y{=}1)}{P(Y{=}0)} = \beta_0 + \sum_j \beta_j x_j \)</div>
      <li>Exponentiating \(\beta_j\) gives odds ratio: \( e^{\beta_j} \).</li>
      <li>Coefficients should be interpreted with feature scaling and collinearity in mind.</li>
    </ul>
  </div>

  <div class="card">
    <h2>4. Decision boundary & visualization</h2>
    <p>
      For two features, logistic regression produces a linear decision boundary (a line). For more features,
      the boundary is a hyperplane. You can add polynomial/interactions to get nonlinear boundaries.
    </p>
    <div class="note">
      <b>Plot idea (Python):</b> Fit model on 2 features and plot probability contour with scatter of classes.
    </div>
  </div>

  <div class="card">
    <h2>5. Practical considerations</h2>
    <ul>
      <li><b>Scaling:</b> Standardize features before regularized logistic regression (L2/L1).</li>
      <li><b>Imbalanced data:</b> use class_weight, resampling, or focus on precision/recall/PR-AUC.</li>
      <li><b>Multicollinearity:</b> inflate variances — consider PCA or remove correlated cols.</li>
      <li><b>Feature selection:</b> L1 (Lasso) can produce sparse coefficients.</li>
    </ul>
  </div>

  <div class="card">
    <h2>6. Hands-on Example A — Diabetes classification (Pima dataset)</h2>

    <p>Steps below include data preparation, training, evaluation, calibration, and interpretation. Copy/paste and run in a Python environment (Jupyter/Colab).</p>

    <pre>
# 1) Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix, classification_report, precision_recall_curve)
import matplotlib.pyplot as plt

# 2) Load data (example: Pima Indians Diabetes CSV)
data = pd.read_csv('diabetes.csv')   # columns include: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age, Outcome

X = data.drop('Outcome', axis=1)
y = data['Outcome']

# 3) Split (stratified to preserve class ratio)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 4) Preprocessing pipeline: scale features (fit on train only)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# 5) Baseline model
clf = LogisticRegression(max_iter=1000, solver='liblinear')   # liblinear works well for small problems
clf.fit(X_train_scaled, y_train)
y_pred = clf.predict(X_test_scaled)
y_proba = clf.predict_proba(X_test_scaled)[:,1]

# 6) Evaluation
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1:', f1_score(y_test, y_pred))
print('ROC-AUC:', roc_auc_score(y_test, y_proba))
print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 7) Precision-Recall curve
prec, rec, thr = precision_recall_curve(y_test, y_proba)
plt.plot(rec, prec)
plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve'); plt.grid(True)
plt.show()

# 8) Coefficients (interpretable)
coef_df = pd.DataFrame({'feature': X.columns, 'coef': clf.coef_[0]})
coef_df.sort_values('coef', ascending=False, inplace=True)
print(coef_df)
    </pre>

    <p><b>Notes & tips:</b></p>
    <ul>
      <li>If dataset has missing/inaccurate zeros in physiological fields (Glucose, BloodPressure, BMI), handle them with domain-aware imputation (median, KNN, or model-based).</li>
      <li>Use <code>class_weight='balanced'</code> or resampling if positives are rare.</li>
      <li>To get calibrated probabilities, use <code>sklearn.calibration.CalibratedClassifierCV</code>.</li>
    </ul>
  </div>

  <div class="card">
    <h2>7. Hands-on Example B — Sales: Predicting High vs Low sales</h2>

    <p>Use logistic regression to predict if next-month sales will be high (1) or low (0) based on marketing & seasonal features.</p>

    <pre>
# sample synthetic example
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

data = pd.DataFrame({
    'TV': [230.1, 44.5, 17.2, 151.5, 180.8, 8.7, 57.5, 120.2],
    'Radio': [37.8, 39.3, 45.9, 41.3, 10.8, 48.9, 32.8, 19.6],
    'Newspaper': [69.2, 45.1, 69.3, 58.5, 58.4, 75.0, 23.5, 11.6],
    'Season': ['Festive','Festive','Off','Off','Festive','Off','Off','Festive'],
    'HighSales': [1,1,0,0,1,0,0,1]
})

X = data.drop('HighSales', axis=1)
y = data['HighSales']

# Preprocessing + model pipeline
numeric_features = ['TV','Radio','Newspaper']
categorical_features = ['Season']

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_features)
])

pipe = Pipeline([
    ('prep', preprocessor),
    ('clf', LogisticRegression(class_weight='balanced', max_iter=500))
])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
pipe.fit(X_train, y_train)
print('Test accuracy:', pipe.score(X_test, y_test))
    </pre>

    <p><b>Extend:</b> Use probabilities to rank customers likely to buy, or set threshold to balance precision vs recall according to business needs.</p>
  </div>

  <div class="card">
    <h2>8. Regularization & Hyperparameter tuning</h2>
    <p>
      Regularization controls overfitting. In scikit-learn logistic regression:
      <ul>
        <li><code>C</code> is inverse regularization strength (smaller C → stronger regularization)</li>
        <li><code>penalty</code> = 'l2' (default) or 'l1' (sparse), or 'elasticnet'</li>
      </ul>
    </p>

    <pre>
# Grid search example
from sklearn.model_selection import GridSearchCV

param_grid = {
  'clf__C': [0.01, 0.1, 1, 10],
  'clf__penalty': ['l2'],
  'clf__solver': ['liblinear']
}

gs = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc')
gs.fit(X_train, y_train)
print(gs.best_params_, gs.best_score_)
    </pre>
  </div>

  <div class="card">
    <h2>9. Limitations & when to use</h2>
    <ul>
      <li>Works best when class boundary is approximately linear in features (or transform features).</li>
      <li>Probabilities can be miscalibrated; use calibration when probabilities drive decisions.</li>
      <li>Not ideal for complex non-linear patterns — consider tree ensembles or neural nets then.</li>
    </ul>
  </div>

  <div class="card try">
    <h2>10. Exercises</h2>
    <ol>
      <li>Run the Pima experiment. Compare performance with & without scaling, and with class weighting.</li>
      <li>Calibrate probabilities and compare Brier score before/after calibration.</li>
      <li>Introduce polynomial features (degree 2) for two informative columns and observe decision boundary changes.</li>
      <li>Report feature coefficients and compute odds ratios; interpret top-3 features for diabetes risk.</li>
    </ol>
  </div>

  <footer style="margin-top:30px;color:#666;font-size:14px">
    Lecture 27 — Logistic Regression | Fundamentals of Machine Learning — © 2025
  </footer>
</body>
</html>
