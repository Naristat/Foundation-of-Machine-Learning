<!-- ========================= lecture_26.html ========================= -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Lecture 25 - Linear Regression</title>
  <style>
    :root{--bg:#f8f9fa;--ink:#222;--brand:#002855;--accent:#0d6efd;--soft:#ffffff;--muted:#6c757d;--warn:#ff8800;--try:#20c997}
    body{font-family:Arial,Helvetica,sans-serif;margin:24px 5vw;background:var(--bg);color:var(--ink);line-height:1.6}
    .nav{display:flex;flex-wrap:wrap;gap:10px;margin:8px 0 24px}
    .nav a{padding:8px 12px;border-radius:8px;background:#fff;border:1px solid #e5e7eb;text-decoration:none;color:#0d6efd}
    .nav a[aria-current="page"]{background:#e7f1ff;border-color:#b6d4fe;color:#084298;font-weight:700}
    h1,h2,h3{color:var(--brand)}
    .card{background:#fff;border-radius:12px;padding:16px;margin:14px 0;box-shadow:0 2px 8px rgba(0,0,0,.06);border-left:5px solid var(--accent)}
    .grid{display:grid;gap:14px}
    @media(min-width:900px){.grid.cols-2{grid-template-columns:1fr 1fr}.grid.cols-3{grid-template-columns:1fr 1fr 1fr}}
    code{background:#eef2f7;border-radius:4px;padding:2px 6px}
    table{width:100%;border-collapse:collapse;background:#fff;border-radius:10px;overflow:hidden}
    th,td{padding:10px;border-bottom:1px solid #e9ecef;text-align:left}
    th{background:#f1f5f9}
    .warn{border-left-color:var(--warn)}
    .try{border-left-color:var(--try)}
    .math{font-family:"Times New Roman",serif;background:#f8f9fc;padding:6px 10px;border-radius:6px;display:inline-block}
  </style>
</head>
<body>
   <nav class="nav">
                    <a href="https://surnamnarendra.github.io/Fundamentals-of-Machine-Learning/" aria-current="page">Home</a>
          <a href="lecture_17.html" aria-current="page">L21</a>
          <a href="lecture_18.html">L22</a>
          <a href="lecture_19.html">L23</a>
          <a href="lecture_20.html">L24</a>
              <a href="lecture_21.html" aria-current="page">L25</a>
          <a href="lecture_22.html">L26</a>
          <a href="lecture_23.html">L27</a>
          <a href="lecture_24.html">L28</a>
          <a href="lecture_25.html" aria-current="page">L29</a>
          <a href="lecture_26.html">L30</a>
        </nav>

  <h1>Lecture 25: Linear Regression</h1>
  <p>
    Linear Regression is one of the most fundamental and widely used algorithms in machine learning and statistics.
    It models the relationship between a dependent variable (target) and one or more independent variables (features)
    by fitting a straight line (or hyperplane) through the data.
  </p>

  <h2>1) Mathematical Foundation</h2>
  <div class="card">
    <p class="math">y = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub> + ε</p>
    <ul>
      <li><b>y</b>: Dependent variable (target)</li>
      <li><b>x<sub>i</sub></b>: Independent variables (features)</li>
      <li><b>w<sub>i</sub></b>: Coefficients (weights) learned by the model</li>
      <li><b>w<sub>0</sub></b>: Intercept term</li>
      <li><b>ε</b>: Error term (residuals)</li>
    </ul>
    <p>The goal is to estimate coefficients <b>w</b> that minimize the squared error:</p>
    <p class="math">J(w) = \(\frac{1}{m}\) ∑ (y<sub>i</sub> - ŷ<sub>i</sub>)²</p>
  </div>

  <h2>2) Assumptions of Linear Regression</h2>
  <table>
    <tr><th>Assumption</th><th>Description</th></tr>
    <tr><td>Linearity</td><td>Relationship between predictors and target is linear.</td></tr>
    <tr><td>Independence</td><td>Observations are independent of each other.</td></tr>
    <tr><td>Homoscedasticity</td><td>Constant variance of errors across values of predictors.</td></tr>
    <tr><td>No multicollinearity</td><td>Predictors should not be highly correlated with each other.</td></tr>
    <tr><td>Normality of errors</td><td>Residuals are normally distributed.</td></tr>
  </table>

  <h2>3) Graphical Representation</h2>
  <div class="grid cols-2">
    <div class="card">
      <b>Simple Linear Regression</b>
      <p>A straight line fitted to data points in 2D (X vs Y).</p>
      <img src="https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg" width="100%" alt="Simple Linear Regression Graph"/>
    </div>
    <div class="card">
      <b>Multiple Linear Regression</b>
      <p>A hyperplane fitted in higher dimensions.</p>
      <img src="https://upload.wikimedia.org/wikipedia/commons/3/3d/Multiple_regression_plane.png" width="100%" alt="Multiple Regression Graph"/>
    </div>
  </div>

  <h2>4) Applications</h2>
  <ul>
    <li><b>Economics:</b> Predicting sales, revenue, or demand based on multiple factors.</li>
    <li><b>Healthcare:</b> Predicting patient health indicators (e.g., blood pressure, BMI).</li>
    <li><b>Business:</b> Forecasting costs, marketing effectiveness, or pricing models.</li>
    <li><b>Science:</b> Modeling relationships between physical variables.</li>
  </ul>

  <h2>5) Hands-on Example 1: Diabetes Classification (Binary → Logistic Extension)</h2>
  <div class="card try">
    <p>Although diabetes prediction is a classification problem, linear regression can be used first as a baseline model by predicting continuous probabilities (then thresholded).</p>
    <pre><code>from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X, y = load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

preds = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, preds))</code></pre>
    <ul>
      <li>Use diabetes dataset (continuous outcome, disease progression).</li>
      <li>Train/test split, evaluate with <b>MSE</b> and <b>R² score</b>.</li>
      <li>Next step → upgrade to Logistic Regression for classification (Lecture 27).</li>
    </ul>
  </div>

  <h2>6) Hands-on Example 2: Sales Forecasting (Continuous Target)</h2>
  <div class="card try">
    <p>We predict <b>monthly sales</b> using advertising spend on TV, radio, and newspaper.</p>
    <pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Example Sales Data
sales = pd.DataFrame({
    "TV":[230.1,44.5,17.2,151.5,180.8],
    "Radio":[37.8,39.3,45.9,41.3,10.8],
    "Newspaper":[69.2,45.1,69.3,58.5,58.4],
    "Sales":[22.1,10.4,9.3,18.5,12.9]
})

X = sales[["TV","Radio","Newspaper"]]
y = sales["Sales"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
model = LinearRegression()
model.fit(X_train,y_train)

preds = model.predict(X_test)
print("R² Score:", r2_score(y_test,preds))</code></pre>
    <p>
      By interpreting coefficients, we can see which medium (TV, Radio, Newspaper) contributes most to sales.
    </p>
  </div>

  <h2>7) Regularization (Ridge, Lasso, ElasticNet)</h2>
  <div class="grid cols-3">
    <div class="card">
      <b>Ridge</b>
      <p>Penalizes large coefficients using L2 norm.</p>
      <p class="math">J = RSS + λ ∑ w²</p>
    </div>
    <div class="card">
      <b>Lasso</b>
      <p>Penalizes absolute values of coefficients (L1 norm) → feature selection.</p>
      <p class="math">J = RSS + λ ∑ |w|</p>
    </div>
    <div class="card">
      <b>ElasticNet</b>
      <p>Combination of Ridge + Lasso.</p>
      <p class="math">J = RSS + λ<sub>1</sub>∑ w² + λ<sub>2</sub>∑ |w|</p>
    </div>
  </div>

  <h2>8) Practical Playbook</h2>
  <div class="card">
    <ul>
      <li>Check assumptions with residual plots.</li>
      <li>Scale features when using regularization.</li>
      <li>Use cross-validation to avoid overfitting.</li>
      <li>Interpret coefficients carefully (units matter).</li>
    </ul>
    <pre><code># Pipeline with scaling + regularization
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("ridge", Ridge(alpha=1.0))
])
pipe.fit(X_train, y_train)</code></pre>
  </div>

  <div class="card try">
    <b>Try This:</b> 
    <ul>
      <li>Fit Linear, Ridge, and Lasso models on the <b>sales dataset</b>.</li>
      <li>Compare R² and MSE values.</li>
      <li>Check which features drop out when using Lasso.</li>
    </ul>
  </div>
</body>
</html>
