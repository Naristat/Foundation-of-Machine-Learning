<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lecture 1: Mathematical Foundation for ML</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; background: #f9f9f9; max-width: 900px; }
    h1, h2 { color: #2c3e50; }
    h3 { color: #34495e; }
    .example { background: #ecf0f1; padding: 10px; border-left: 4px solid #3498db; margin: 15px 0; }
    code { background: #eee; padding: 2px 6px; border-radius: 4px; }
    .nav { margin-top: 30px; }
    .nav a { margin-right: 15px; text-decoration: none; color: #3498db; }
    .nav a:hover { text-decoration: underline; }
  </style>
</head>
<body>

  <h1>Lecture 1: Mathematical Foundation for Machine Learning</h1>

  <h2>1. Linear Algebra Basics</h2>
  <p>Matrices and vectors are fundamental in machine learning. Almost every dataset and model can be represented using vectors and matrices.</p>

  <h3>1.1 Matrix Definition</h3>
  <p>A matrix is a rectangular array of numbers:</p>
  <div class="example">
    \( A = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{bmatrix} \)
  </div>
  <p><strong>ML Application:</strong> Dataset representation \( X \in \mathbb{R}^{m \times n} \), each row is a data point, each column a feature.</p>

  <h3>1.2 Common Operations</h3>
  <ul>
    <li><strong>Addition:</strong> \( C = A + B \)</li>
    <li><strong>Scalar Multiplication:</strong> \( B = kA \)</li>
    <li><strong>Matrix Multiplication:</strong> \( C = AB \)</li>
    <li><strong>Transpose:</strong> \( A^T \)</li>
    <li><strong>Inverse:</strong> \( A^{-1} \)</li>
  </ul>
  <div class="example">
    Example: <br>
    \( A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, 
       B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} \) <br>
    \( A + B = \begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}, \quad
       AB = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix} \)
  </div>
  <p><strong>ML Application:</strong> Neural network forward propagation uses matrix multiplication to compute weighted sums.</p>

  <h2>2. Vector Spaces in Machine Learning</h2>
  <ul>
    <li>Data points: \( x \in \mathbb{R}^n \)</li>
    <li>Weights: \( w \in \mathbb{R}^n \)</li>
    <li>Dot product: \( w^T x \)</li>
    <li>Norms: \( \|x\|_2 = \sqrt{\sum_i x_i^2} \)</li>
  </ul>
  <p><strong>ML Applications:</strong> Cosine similarity in NLP, L2 regularization in regression, distance metrics in clustering.</p>

  <h2>3. Probability Basics</h2>
  <ul>
    <li>Random variable \( X \) – represents outcomes</li>
    <li>Probability distribution \( P(X) \)</li>
    <li>Conditional probability \( P(Y|X) \)</li>
    <li>Expectation \( \mathbb{E}[X] \)</li>
    <li>Variance \( \text{Var}(X) \)</li>
  </ul>
  <p><strong>ML Applications:</strong> Naive Bayes, expected loss minimization, bias-variance tradeoff, uncertainty estimation.</p>

  <h2>4. Eigenvalues and Eigenvectors</h2>
  <p>\( Av = \lambda v \)</p>
  <p><strong>ML Applications:</strong> PCA for dimensionality reduction, data compression, noise reduction, feature extraction.</p>

  <h2>5. Calculus in Machine Learning</h2>
  <p>Calculus is essential in optimization, computing gradients, and training models.</p>

  <h3>5.1 Derivatives</h3>
  <p>Derivative measures how a function changes with respect to a variable:</p>
  <div class="example">
    \( f(x) = x^2 \quad \Rightarrow \quad f'(x) = 2x \)
  </div>
  <p><strong>ML Application:</strong> Gradient of loss functions guides the optimization in linear regression, logistic regression, and neural networks.</p>

  <h3>5.2 Partial Derivatives</h3>
  <p>For functions of multiple variables, partial derivatives measure change w.r.t. one variable:</p>
  <div class="example">
    \( f(x, y) = x^2 + y^2 \quad \Rightarrow \quad \frac{\partial f}{\partial x} = 2x, \quad \frac{\partial f}{\partial y} = 2y \)
  </div>
  <p><strong>ML Application:</strong> Used in gradient descent for multi-parameter models, e.g., weights in neural networks.</p>

  <h3>5.3 Gradients</h3>
  <p>The gradient is a vector of partial derivatives:</p>
  <div class="example">
    \( \nabla f(x, y) = \begin{bmatrix} \frac{\partial f}{\partial x} \\[2mm] \frac{\partial f}{\partial y} \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \end{bmatrix} \)
  </div>
  <p><strong>ML Application:</strong> Gradient vector shows the direction of steepest ascent; negative gradient is used in gradient descent to minimize loss.</p>

  <h3>5.4 Chain Rule</h3>
  <p>Used to compute derivatives of composite functions:</p>
  <div class="example">
    \( f(g(x)) \quad \Rightarrow \quad \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx} \)
  </div>
  <p><strong>ML Application:</strong> Backpropagation in neural networks relies on the chain rule to propagate gradients through layers.</p>

  <h2>6. Applications in Machine Learning</h2>
  <ul>
    <li>Linear regression: derivative of MSE loss w.r.t. weights</li>
    <li>Logistic regression: derivative of log-loss for probability prediction</li>
    <li>Neural networks: backpropagation and weight updates using gradients</li>
    <li>Optimization: gradient descent, stochastic gradient descent, Adam optimizer</li>
    <li>PCA: eigen decomposition combined with derivatives to find principal components</li>
  </ul>

  <div class="nav">
    <a href="lecture_2.html">Next Lecture →</a>
  </div>

</body>
</html>
