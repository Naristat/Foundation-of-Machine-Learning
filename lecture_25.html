<!DOCTYPE html>
<html lang="en">
  </div>
        <nav class="nav">
                    <a href="https://surnamnarendra.github.io/Fundamentals-of-Machine-Learning/" aria-current="page">Home</a>
          <a href="lecture_17.html" aria-current="page">L21</a>
          <a href="lecture_18.html">L22</a>
          <a href="lecture_19.html">L23</a>
          <a href="lecture_20.html">L24</a>
              <a href="lecture_21.html" aria-current="page">L25</a>
          <a href="lecture_22.html">L26</a>
          <a href="lecture_23.html">L27</a>
          <a href="lecture_24.html">L28</a>
          <a href="lecture_25.html" aria-current="page">L29</a>
          <a href="lecture_26.html">L30</a>
        </nav>
      </div>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Lecture 29 — SVM Dual Formulation & Kernels</title>
  <style>
    :root{
      --bg:#f6fbff; --card:#ffffff; --muted:#6b7280; --accent:#0b5cff; --accent-2:#059669; --title:#07203a;
    }
    *{box-sizing:border-box}
    body{
      margin:0; font-family: Inter, system-ui, -apple-system, "Segoe UI", Roboto, Arial;
      background: linear-gradient(180deg,#f7fbff 0%, #ffffff 100%); color:#0f172a;
      -webkit-font-smoothing:antialiased;
    }
    .wrap{max-width:1100px;margin:28px auto;padding:20px;}
    header{display:flex;align-items:center;justify-content:space-between;gap:12px;margin-bottom:18px;}
    header h1{margin:0;font-size:1.4rem;color:var(--title)}
    nav a{color:var(--accent);text-decoration:none;margin-left:12px;font-size:0.95rem}
    .card{background:var(--card);border-radius:12px;padding:18px;box-shadow:0 6px 20px rgba(15,23,42,0.06);border:1px solid rgba(15,23,42,0.04)}
    main{display:grid;grid-template-columns:1fr 380px;gap:20px}
    @media(max-width:980px){ main{grid-template-columns:1fr} .aside{order:2} }
    h2{color:var(--accent);margin-top:0}
    p{color:#374151}
    pre{background:#0b1220;color:#e6eefc;padding:12px;border-radius:8px;overflow:auto;font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", monospace;font-size:0.9rem}
    ul{margin:8px 0 14px 20px}
    .models{display:grid;grid-template-columns:1fr 1fr;gap:12px}
    .model{padding:12px;border-radius:8px;border:1px solid rgba(9,30,66,0.04);background:#fff}
    label{display:block;margin:8px 0 6px;font-weight:600}
    input[type="number"], input[type="text"], select{width:100%;padding:8px;border-radius:8px;border:1px solid rgba(15,23,42,0.08);font-size:0.95rem}
    button{padding:10px 14px;border-radius:8px;border:0;background:var(--accent);color:white;font-weight:700;cursor:pointer}
    .btn-muted{background:#6b7280}
    .muted{color:var(--muted);font-size:0.92rem}
    .result{margin-top:12px;padding:12px;border-radius:8px;background:#0f172a;color:#fff}
    table{width:100%;border-collapse:collapse;margin-top:8px}
    th,td{padding:6px;border-bottom:1px solid #eef2f7;text-align:left;font-size:0.95rem}
    .small{font-size:0.9rem;color:var(--muted)}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div>
        <h1>Lecture 29 — SVM Dual Formulation & Kernels</h1>
        <div class="small muted">Dual problem • KKT conditions • Kernel trick • Practical tips • Interactive demo</div>
      </div>
      <nav>
        <a href="#dual">Dual</a>
        <a href="#kkt">KKT</a>
        <a href="#kernels">Kernels</a>
        <a href="#demo">Demo</a>
      </nav>
    </header>

    <main>
      <section class="card">
        <h2 id="dual">1. From Primal to Dual (intuition + derivation)</h2>

        <p><strong>Primal (soft-margin SVM):</strong> for training examples (xᵢ, yᵢ), yᵢ∈{−1,+1},</p>
        <pre><code>min_{w,b,ξ}  (1/2) ||w||² + C ∑_{i} ξᵢ
s.t.  yᵢ (w·xᵢ + b) ≥ 1 - ξᵢ,   ξᵢ ≥ 0  for all i
</code></pre>

        <p>Introduce Lagrange multipliers αᵢ ≥ 0 for margin constraints and μᵢ ≥ 0 for ξᵢ ≥ 0. Solve stationarity conditions → eliminate primal variables w, ξ, b. This yields the <strong>dual</strong>:</p>

        <pre><code>max_{α}  ∑_{i} αᵢ − (1/2) ∑_{i,j} αᵢ αⱼ yᵢ yⱼ (xᵢ·xⱼ)
s.t.   0 ≤ αᵢ ≤ C,   ∑_{i} αᵢ yᵢ = 0
</code></pre>

        <p><strong>Key observations:</strong></p>
        <ul>
          <li>Only training points appear through inner-products xᵢ·xⱼ (the Gram matrix).</li>
          <li>Support vectors are those with αᵢ > 0; points with 0 < αᵢ < C lie exactly on margin (support), αᵢ = C are slack-support (misclassified/within margin).</li>
          <li>After solving α, compute <code>w = ∑ αᵢ yᵢ xᵢ</code> and find <code>b</code> using support vectors.</li>
        </ul>

        <h2 id="kkt">2. KKT Conditions (optimality)</h2>
        <p>The Karush-Kuhn-Tucker conditions link primal and dual optimal solutions:</p>
        <ul>
          <li>Primal feasibility: constraints satisfied.</li>
          <li>Dual feasibility: αᵢ ≥ 0 and μᵢ ≥ 0.</li>
          <li>Stationarity: gradient of Lagrangian w.r.t primal = 0 → yields w = ∑ αᵢ yᵢ xᵢ.</li>
          <li>Complementary slackness: αᵢ [ yᵢ (w·xᵢ + b) − 1 + ξᵢ ] = 0 and μᵢ ξᵢ = 0.</li>
        </ul>

        <p>Complementary slackness implies:</p>
        <ul>
          <li>If αᵢ > 0 → the corresponding constraint is active: yᵢ (w·xᵢ + b) = 1 − ξᵢ.</li>
          <li>If 0 < αᵢ < C → ξᵢ = 0 and point lies exactly on margin (distance = 1/||w||).</li>
        </ul>

        <h2 id="kernels">3. Kernel Trick</h2>
        <p>Replace inner product <code>xᵢ·xⱼ</code> by a kernel function <code>K(xᵢ,xⱼ)=φ(xᵢ)·φ(xⱼ)</code> computed directly in input space. Dual becomes:</p>
        <pre><code>max_{α}  ∑ αᵢ − (1/2) ∑ αᵢ αⱼ yᵢ yⱼ K(xᵢ,xⱼ)
s.t.  0 ≤ αᵢ ≤ C,  ∑ αᵢ yᵢ = 0
</code></pre>

        <p>Common kernels:</p>
        <div class="models" style="margin-top:8px">
          <div class="model"><strong>Linear:</strong> K(u,v)=u·v</div>
          <div class="model"><strong>Polynomial:</strong> K(u,v)=(γ u·v + r)^d</div>
          <div class="model"><strong>RBF / Gaussian:</strong> K(u,v)=exp(−γ ||u−v||²)</div>
          <div class="model"><strong>Sigmoid:</strong> K(u,v)=tanh(γ u·v + r)</div>
        </div>

        <p class="small">Use kernels when you suspect a nonlinear decision boundary but want to avoid explicit mapping φ(x).</p>

        <h2 id="practical">4. Practical Notes</h2>
        <ul>
          <li>Solve the dual via quadratic programming (QP) — many libraries (libsvm, scikit-learn) use SMO or specialized solvers.</li>
          <li>Scale features — kernels are sensitive to feature scales.</li>
          <li>Choose C (tradeoff margin vs errors) and kernel hyperparameters (γ, degree) via cross-validation.</li>
          <li>For large datasets prefer linear SVM (liblinear) or approximate methods / subsampling.</li>
        </ul>

        <h2 id="code">5. Python sketch (scikit-learn)</h2>
        <pre><code># train SVM with RBF kernel
from sklearn.svm import SVC
clf = SVC(C=1.0, kernel='rbf', gamma='scale')
clf.fit(X_train, y_train)

# support vectors and dual coefficients:
sv = clf.support_vectors_
alphas = clf.dual_coef_   # shape (n_classes-1, n_SV)
intercept = clf.intercept_
        </code></pre>

        <p class="small">Note: scikit-learn hides the QP solver; libsvm under the hood implements SMO and stores dual coefficients.</p>
      </section>

      <aside class="card aside">
        <h3 class="small muted">Interactive Dual / Kernel Demo</h3>
        <p class="small">Enter up to 4 training points (2D), their labels (±1), and dual coefficients α (toy values). Choose a kernel and evaluate decision function for a query point. This helps see how α, labels, and kernels combine to form the classifier:</p>

        <label>Kernel</label>
        <select id="kernel">
          <option value="linear">Linear</option>
          <option value="poly">Polynomial (d=2)</option>
          <option value="rbf">RBF (γ=0.5)</option>
        </select>

        <label>Training points (x,y,label,alpha) — up to 4 lines</label>
        <textarea id="svInput" rows="6" placeholder="e.g.
1,1,1,0.8
2,1,-1,0.5
-1,-1,1,0.0
-2,0,-1,0.0"></textarea>

        <label>Query point (x,y)</label>
        <input id="qx" type="number" placeholder="e.g. 0.5">
        <input id="qy" type="number" placeholder="e.g. 0.0">

        <button onclick="evaluateDual()">Evaluate Decision</button>

        <div id="dualResult" class="result" style="display:none"></div>

        <p class="small muted" style="margin-top:8px">This demo expects you to supply the α coefficients (toy). In practice α are found by solving the dual QP.</p>
      </aside>
    </main>

    <footer style="margin-top:18px" class="card">
      <strong>Next:</strong> tuning SVM hyperparameters (grid search), multi-class SVM strategies, and scaling to large data.
    </footer>
  </div>

  <script>
    // Kernel implementations
    function kernelVal(k, x, y){
      // x, y are objects {x, y}
      if(k === 'linear'){
        return x.x * y.x + x.y * y.y - 0; // careful: must use y.y but we'll pass point
      }
      if(k === 'poly'){
        const dot = x.x * y.x + x.y * y.y;
        const gamma = 1.0;
        const r = 0.0;
        const d = 2;
        return Math.pow(gamma * dot + r, d);
      }
      if(k === 'rbf'){
        const gamma = 0.5;
        const dx = x.x - y.x;
        const dy = x.y - y.y;
        return Math.exp(-gamma * (dx*dx + dy*dy));
      }
      return x.x * y.x + x.y * y.y;
    }

    // Note: above kernelVal has a bug if used as-is because it mixes x and y names.
    // We'll implement a proper kernel function below.

    function K(k, a, b){
      if(k === 'linear'){
        return a.x * b.x + a.y * b.y;
      }
      if(k === 'poly'){
        const dot = a.x * b.x + a.y * b.y;
        const gamma = 1.0;
        const r = 0.0;
        const d = 2;
        return Math.pow(gamma * dot + r, d);
      }
      if(k === 'rbf'){
        const gamma = 0.5;
        const dx = a.x - b.x;
        const dy = a.y - b.y;
        return Math.exp(-gamma * (dx*dx + dy*dy));
      }
      return a.x * b.x + a.y * b.y;
    }

    function parseSVInput(text){
      const lines = text.trim().split('\n').map(l => l.trim()).filter(Boolean);
      const pts = [];
      for(const line of lines){
        const parts = line.split(',').map(s=>s.trim());
        if(parts.length < 4) continue;
        const x = parseFloat(parts[0]);
        const y = parseFloat(parts[1]);
        const label = parseFloat(parts[2]);
        const alpha = parseFloat(parts[3]);
        if(isNaN(x) || isNaN(y) || (label !== 1 && label !== -1) || isNaN(alpha)) continue;
        pts.push({x, y, label, alpha});
      }
      return pts;
    }

    function evaluateDual(){
      const kernel = document.getElementById('kernel').value;
      const svText = document.getElementById('svInput').value;
      const qx = parseFloat(document.getElementById('qx').value);
      const qy = parseFloat(document.getElementById('qy').value);

      const pts = parseSVInput(svText);
      if(pts.length === 0){
        alert('Enter at least one valid training line: x,y,label(+1/-1),alpha');
        return;
      }
      if(isNaN(qx) || isNaN(qy)){
        alert('Enter query x and y.');
        return;
      }
      const q = {x: qx, y: qy};

      // decision function f(q) = sum_i alpha_i * y_i * K(x_i, q) + b
      // b isn't known from alpha alone unless computed; here we approximate b using one support vector with 0 < alpha < C (if present)
      let f = 0;
      for(const p of pts){
        f += p.alpha * p.label * K(kernel, {x:p.x, y:p.y}, q);
      }

      // Attempt to estimate b: pick a point with 0 < alpha < C if present. For demo we assume C large and treat alpha as exact SV.
      // Use b = y_s - sum_j alpha_j y_j K(x_j, x_s)
      let b = null;
      for(const s of pts){
        if(s.alpha > 1e-9 && s.alpha < 1e3){ // demo heuristic
          let sum = 0;
          for(const j of pts){
            sum += j.alpha * j.label * K(kernel, {x:j.x, y:j.y}, {x:s.x, y:s.y});
          }
          b = s.label - sum;
          break;
        }
      }
      if(b === null) b = 0.0;

      const decisionScore = f + b;
      const pred = decisionScore >= 0 ? 1 : -1;

      const resBox = document.getElementById('dualResult');
      resBox.style.display = 'block';
      resBox.innerHTML = `<strong>Decision function f(q):</strong> ${decisionScore.toFixed(6)} <br>
                         <strong>Predicted label:</strong> ${pred} <br>
                         <strong>Used ${pts.length} points (alpha provided by user).</strong>
                         <div style="margin-top:8px"><strong>Details:</strong><table style="width:100%"><thead><tr><th>i</th><th>x,y</th><th>label</th><th>alpha</th><th>K(x_i,q)</th><th>contribution</th></tr></thead><tbody>` +
                         pts.map((p,i)=>`<tr><td>${i+1}</td><td>(${p.x}, ${p.y})</td><td>${p.label}</td><td>${p.alpha}</td><td>${K(kernel,{x:p.x,y:p.y},q).toFixed(6)}</td><td>${(p.alpha*p.label*K(kernel,{x:p.x,y:p.y},q)).toFixed(6)}</td></tr>`).join('') +
                         `</tbody></table></div><div style="margin-top:8px"><strong>Estimated b:</strong> ${b.toFixed(6)} (approx)</div>`;
    }
  </script>
</body>
</html>
