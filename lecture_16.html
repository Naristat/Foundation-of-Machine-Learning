<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width,initial-scale=1"/>
  <title>Lecture 16 — Logistic Regression & Examples</title>
  <style>
    :root{--bg:#f6f8fb; --card:#fff; --accent:#0b6cf0; --muted:#243b4a;}
    body{font-family:Inter,system-ui,-apple-system,'Segoe UI',Roboto,Arial; background:var(--bg); color:var(--muted); margin:0;padding:24px}
    .wrap{max-width:1000px;margin:0 auto}
    header{display:flex;align-items:center;gap:12px}
    h1{margin:0;color:#0f1724}
    .meta{color:#64748b;font-size:0.95rem}
    .card{background:var(--card);border-radius:12px;padding:18px;margin-top:18px;box-shadow:0 8px 20px rgba(2,6,23,0.06)}
    h2{color:#0f1724}
    pre{background:#0f1724;color:#eef6ff;padding:12px;border-radius:8px;overflow:auto}
    code{background:#eef4ff;padding:2px 6px;border-radius:6px}
    .grid{display:grid;grid-template-columns:1fr 360px;gap:18px}
    label{display:block;margin-top:8px;font-weight:600}
    input[type=number], textarea, select {width:100%;padding:8px;border-radius:8px;border:1px solid #e6eef8}
    button{margin-top:10px;padding:10px 14px;border-radius:10px;border:none;background:var(--accent);color:#fff;cursor:pointer}
    .note{font-size:0.95rem;color:#475569}
    .result{background:#f8fafc;padding:12px;border-radius:8px;margin-top:10px;white-space:pre-wrap;font-family:monospace}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div>
        <h1>Lecture 16 — Logistic Regression & Examples</h1>
        <div class="meta">Binary & Multiclass logistic regression, loss, optimization (GD, SGD, Newton/IRLS), regularization, evaluation & examples</div>
      </div>
    </header>

    <section class="card">
      <h2>Overview</h2>
      <p class="note">Logistic regression models probability of class membership using a sigmoid (binary) or softmax (multiclass). Training minimizes cross-entropy loss. Optimization is commonly done via (stochastic) gradient descent or Newton-like methods (IRLS) when data is moderate.</p>
    </section>

    <section class="card">
      <h2>1. Binary Logistic Regression — Theory</h2>
      <p>
        Model: for input vector x (including intercept), define linear score z = wᵀx.  
        Probability of class 1:
      </p>
      <p style="text-align:center;font-weight:700">p(y=1|x) = σ(z) = 1 / (1 + e^{-z})</p>
      <p>Cross-entropy loss (negative log-likelihood) for dataset {(xᵢ, yᵢ)}:</p>
      <pre>
L(w) = - Σ [ yᵢ log σ(wᵀxᵢ) + (1-yᵢ) log(1 - σ(wᵀxᵢ)) ]
      </pre>
      <p>Gradient:</p>
      <pre>
∇L(w) = Σ (σ(wᵀxᵢ) - yᵢ) xᵢ
      </pre>
      <p>Use this gradient in GD/SGD updates: w ← w - η ∇L(w).</p>
    </section>

    <section class="card">
      <h2>2. Multiclass: Softmax & Cross-Entropy</h2>
      <p>For K classes, param matrix W (K×p) gives scores z_k = w_kᵀ x. Softmax:</p>
      <pre>
p(y=k|x) = exp(z_k) / Σ_j exp(z_j)
      </pre>
      <p>Loss (cross-entropy):</p>
      <pre>
L(W) = - Σ_i Σ_k 1{yᵢ=k} log p(yᵢ=k | xᵢ)
      </pre>
      <p>Gradient per class: ∇_{w_k} = Σ_i (p(y=k|xᵢ) - 1{yᵢ=k}) xᵢ</p>
    </section>

    <section class="card">
      <h2>3. Optimization Methods</h2>
      <ul>
        <li><b>Batch Gradient Descent:</b> compute gradient over full dataset each update. Good for small datasets.</li>
        <li><b>Stochastic Gradient Descent (SGD):</b> update per-sample — noisy but scalable.</li>
        <li><b>Mini-batch SGD:</b> update on small batches (typical in DL).</li>
        <li><b>Newton-Raphson / IRLS (Iteratively Reweighted Least Squares):</b> uses Hessian for faster local convergence. Each iteration solves a weighted least-squares problem. Good for moderate-size problems.</li>
        <li><b>Regularized optimization:</b> add λ||w||₂² (L2) or λ||w||₁ (L1). L2 is common (ridge-like), L1 leads to sparse solutions (coordinate descent often used).</li>
      </ul>
    </section>

    <section class="card">
      <h2>4. Numerical Recipes & Practical Tips</h2>
      <ul>
        <li>Feature scaling helps convergence (standardize features).</li>
        <li>Use learning-rate schedules or adaptive optimizers (Adam) for faster training.</li>
        <li>For highly imbalanced classes, use class weights or resampling.</li>
        <li>Monitor metrics: accuracy, precision, recall, F1, AUC-ROC for binary classification.</li>
      </ul>
    </section>

    <section class="card">
      <h2>5. Code snippets (NumPy & scikit-learn)</h2>

      <h3>NumPy — simple batch gradient descent (binary)</h3>
      <pre>
import numpy as np

def sigmoid(z): return 1 / (1 + np.exp(-z))

# X: n x p (include column of ones for intercept), y: n (0/1)
def logistic_gd(X, y, lr=0.1, iters=1000):
    n, p = X.shape
    w = np.zeros(p)
    for t in range(iters):
        z = X @ w
        preds = sigmoid(z)
        grad = X.T @ (preds - y)    # shape (p,)
        w -= lr * grad / n
    return w
      </pre>

      <h3>scikit-learn — quick fit</h3>
      <pre>
from sklearn.linear_model import LogisticRegression

# L2 regularized logistic regression (liblinear/saga solvers)
clf = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0, max_iter=1000)
clf.fit(X_train, y_train)
print(clf.coef_, clf.intercept_)
      </pre>
    </section>

    <section class="card">
      <h2>6. Newton-Raphson / IRLS (brief)</h2>
      <p>
        Newton update: w ← w - H^{-1} g, where g is gradient and H is Hessian. For logistic regression the Hessian is:
      </p>
      <pre>
H = Xᵀ R X, where R is diagonal matrix with rᵢ = σ(zᵢ)(1-σ(zᵢ))
      </pre>
      <p>IRLS solves at each step: (Xᵀ R X) Δw = Xᵀ (y - p) and updates w ← w + Δw. Converges fast when n is moderate; expensive for large p/n because of matrix solves.</p>
    </section>

    <section class="card">
      <h2>7. Worked numeric example (small) — classification boundary</h2>
      <pre>
Toy dataset:
X = [[1, 0.5], [1, 2.5], [1, 1.0], [1, 3.0]]  # first column intercept
y = [0, 1, 0, 1]

Run logistic_gd and inspect w (intercept & slope). Predict probabilities via sigmoid(X @ w).
      </pre>
    </section>

    <section class="card">
      <h2>8. Interactive Playground — Binary Logistic Regression (GD)</h2>
      <p class="note">Enter a small 2D toy dataset (x, y) each line: feature_value,label(0/1). This demo fits w0 + w1 * x using batch gradient descent and prints iterations.</p>

      <div class="grid">
        <div>
          <label>Data (x,label) one per line — example below:</label>
          <textarea id="ldata" rows="6">0.5,0
2.5,1
1.0,0
3.0,1</textarea>

          <label>Learning rate</label>
          <input id="lrate" type="number" value="0.5" step="0.01">

          <label>Iterations</label>
          <input id="iters" type="number" value="30">

          <button onclick="runLogisticGD()">Run Logistic GD</button>
          <div id="lout" class="result"></div>
        </div>

        <aside class="card">
          <h3 style="margin-top:0">Guide</h3>
          <ol>
            <li>Try default dataset, then change points to see decision boundary shift.</li>
            <li>Watch weights and loss (cross-entropy) decrease with iterations.</li>
            <li>Try smaller/larger learning rates to see stability / divergence.</li>
          </ol>
        </aside>
      </div>

      <script>
        function parseDataLog(text){
          const rows = text.trim().split('\\n').map(r=>r.trim()).filter(Boolean);
          const X = [], y=[];
          for(const row of rows){
            const parts = row.split(',').map(s=>s.trim());
            if(parts.length !== 2) return null;
            const xv = parseFloat(parts[0]), yv = parseInt(parts[1]);
            if(Number.isNaN(xv) || Number.isNaN(yv)) return null;
            X.push([1, xv]); // intercept
            y.push(yv);
          }
          return {X, y};
        }

        function sigmoid(z){ return 1 / (1 + Math.exp(-z)); }

        function crossEntropy(y, p){
          // y: array, p: array
          let n = y.length, s = 0;
          for(let i=0;i<n;i++){
            const pi = Math.min(Math.max(p[i], 1e-12), 1-1e-12);
            s += - (y[i]*Math.log(pi) + (1-y[i])*Math.log(1-pi));
          }
          return s / n;
        }

        function runLogisticGD(){
          const raw = document.getElementById('ldata').value;
          const D = parseDataLog(raw);
          const out = document.getElementById('lout');
          if(!D){ out.innerText = 'Invalid data format. Use lines like: 0.5,0'; return; }
          const X = D.X, y = D.y, n = y.length;
          let w0 = 0, w1 = 0;
          const lr = parseFloat(document.getElementById('lrate').value) || 0.1;
          const iters = parseInt(document.getElementById('iters').value) || 50;
          out.innerText = '';
          for(let t=0; t<iters; t++){
            const preds = [];
            let g0 = 0, g1 = 0;
            for(let i=0;i<n;i++){
              const z = w0 + w1 * X[i][1];
              const p = sigmoid(z);
              preds.push(p);
              const err = p - y[i];
              g0 += err;
              g1 += err * X[i][1];
            }
            g0 /= n; g1 /= n;
            w0 -= lr * g0;
            w1 -= lr * g1;
            if(t % Math.max(1, Math.floor(iters/10)) === 0){
              const loss = crossEntropy(y, preds);
              out.innerText += `iter ${t}: w0=${w0.toFixed(4)}, w1=${w1.toFixed(4)}, loss=${loss.toFixed(4)}\\n`;
            }
          }
          // final
          const final_preds = X.map(r => sigmoid(w0 + w1 * r[1]));
          const final_loss = crossEntropy(y, final_preds);
          out.innerText += `final: w0=${w0.toFixed(4)}, w1=${w1.toFixed(4)}, loss=${final_loss.toFixed(4)}\\n`;
          // show probabilities per point
          for(let i=0;i<n;i++){
            out.innerText += `x=${X[i][1]}, y=${y[i]}, p=${final_preds[i].toFixed(3)}\\n`;
          }
        }
      </script>
    </section>

    <section class="card">
      <h2>9. Other Examples & Extensions</h2>
      <ul>
        <li><b>Regularized Logistic Regression:</b> add λ||w||₂² to loss — in code add + λ w to gradient (or use sklearn's C parameter).</li>
        <li><b>Multiclass (Softmax):</b> extend to K classes — use cross-entropy with softmax and optimize with SGD or LBFGS.</li>
        <li><b>Imbalanced data:</b> use class weights in loss (sklearn: class_weight) or focal loss for deep nets.</li>
        <li><b>Probabilistic interpretation:</b> logistic regression is a generalized linear model (GLM) with logit link.</li>
      </ul>
    </section>

    <section class="card">
      <h2>10. Exercises</h2>
      <ol>
        <li>Implement Newton-Raphson / IRLS for the toy dataset and compare convergence to GD.</li>
        <li>Train logistic regression on a real dataset (e.g., Iris binary problem) and compute ROC AUC.</li>
        <li>Compare L2 vs L1 regularization; show effect on coefficients for correlated features.</li>
      </ol>
    </section>

    <footer style="margin-top:18px;text-align:center;color:#64748b">
    </footer>
  </div>
</body>
</html>

</head>

</html>
